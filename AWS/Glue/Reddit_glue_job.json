{
	"jobConfig": {
		"name": "Reddit_glue_job",
		"description": "",
		"role": "arn:aws:iam::438465132279:role/service-role/AWSGlueServiceRole-viswa",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "Reddit_glue_job.py",
		"scriptLocation": "s3://aws-glue-assets-438465132279-us-east-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-12-04T13:57:00.353Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-438465132279-us-east-1/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-438465132279-us-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom awsgluedq.transforms import EvaluateDataQuality\r\nfrom pyspark.sql.functions import concat_ws\r\nfrom awsglue import DynamicFrame\r\n\r\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args['JOB_NAME'], args)\r\n\r\n# Default ruleset used by all target nodes with data quality enabled\r\nDEFAULT_DATA_QUALITY_RULESET = \"\"\"\r\n    Rules = [\r\n        ColumnCount > 0\r\n    ]\r\n\"\"\"\r\n\r\n# Script generated for node Reddit_raw\r\nReddit_raw_node1733319211753 = glueContext.create_dynamic_frame.from_options(\r\n    format_options={\"quoteChar\": \"\\\"\", \"withHeader\": True, \"separator\": \",\"},\r\n    connection_type=\"s3\",\r\n    format=\"csv\",\r\n    connection_options={\"paths\": [\"s3://reddit-api-project/raw/\"], \"recurse\": True},\r\n    transformation_ctx=\"Reddit_raw_node1733319211753\"\r\n)\r\n\r\n# Convert Dynamic Frame to DataFrame\r\ndf = Reddit_raw_node1733319211753.toDF()\r\n\r\n# Concatenate three columns into one\r\ndf_combined = df.withColumn('ESS_combine', concat_ws('-', df['edited'], df['spoiler'], df['stickied']))\r\ndf_combined = df_combined.drop('edited', 'spoiler', 'stickied')\r\n\r\n# Convert back to DynamicFrame\r\ns3_node_combined = DynamicFrame.fromDF(df_combined, glueContext, 's3_node_combined')\r\n\r\n# Evaluate Data Quality\r\nEvaluateDataQuality().process_rows(\r\n    frame=s3_node_combined,\r\n    ruleset=DEFAULT_DATA_QUALITY_RULESET,\r\n    publishing_options={\"dataQualityEvaluationContext\": \"EvaluateDataQuality_node1733319164918\", \"enableDataQualityResultsPublishing\": True},\r\n    additional_options={\"dataQualityResultsPublishing.strategy\": \"BEST_EFFORT\", \"observations.scope\": \"ALL\"}\r\n)\r\n\r\n# Write the transformed DynamicFrame to S3\r\nReddit_transformed_node1733319311361 = glueContext.write_dynamic_frame.from_options(\r\n    frame=s3_node_combined,  # Use s3_node_combined, not Reddit_raw_node1733319211753\r\n    connection_type=\"s3\",\r\n    format=\"glueparquet\",\r\n    connection_options={\"path\": \"s3://reddit-api-project/transformed/\", \"partitionKeys\": []},\r\n    format_options={\"compression\": \"snappy\"},\r\n    transformation_ctx=\"Reddit_transformed_node1733319311361\"\r\n)\r\n\r\njob.commit()\r\n"
}