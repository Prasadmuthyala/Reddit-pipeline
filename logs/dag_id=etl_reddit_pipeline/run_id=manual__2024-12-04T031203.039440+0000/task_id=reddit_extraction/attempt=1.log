[2024-12-04T03:12:11.869+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-12-04T03:12:03.039440+00:00 [queued]>
[2024-12-04T03:12:11.883+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-12-04T03:12:03.039440+00:00 [queued]>
[2024-12-04T03:12:11.884+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 1
[2024-12-04T03:12:11.905+0000] {taskinstance.py:2191} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2024-12-04 03:12:03.039440+00:00
[2024-12-04T03:12:11.918+0000] {standard_task_runner.py:60} INFO - Started process 1558 to run task
[2024-12-04T03:12:11.923+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_extraction', 'manual__2024-12-04T03:12:03.039440+00:00', '--job-id', '18', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmp931e_wo_']
[2024-12-04T03:12:11.925+0000] {standard_task_runner.py:88} INFO - Job 18: Subtask reddit_extraction
[2024-12-04T03:12:11.999+0000] {task_command.py:423} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-12-04T03:12:03.039440+00:00 [running]> on host 67b23099d8b1
[2024-12-04T03:12:12.106+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='viswa' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-12-04T03:12:03.039440+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-12-04T03:12:03.039440+00:00'
[2024-12-04T03:12:14.114+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': "Based on [last year's thread](https://www.reddit.com/r/dataengineering/comments/18kt2fc/2024_data_engineering_top_skills_that_you_will/), let's see if the most relevant DE tech stacks have changed, as this niche moves so fast:\n\n\nAre you thinking about getting new skills? What will you suggest if you want to be a updated data engineer or data manager?\n\nAny certifications? Any courses? Any local or enterprise projects? Any ideas to launch your personal brand?", 'author_fullname': 't2_ezqgf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '2025 Data Engineering Top Skills that you will prepare for', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5shzy', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 86, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 86, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733246082.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on &lt;a href="https://www.reddit.com/r/dataengineering/comments/18kt2fc/2024_data_engineering_top_skills_that_you_will/"&gt;last year&amp;#39;s thread&lt;/a&gt;, let&amp;#39;s see if the most relevant DE tech stacks have changed, as this niche moves so fast:&lt;/p&gt;\n\n&lt;p&gt;Are you thinking about getting new skills? What will you suggest if you want to be a updated data engineer or data manager?&lt;/p&gt;\n\n&lt;p&gt;Any certifications? Any courses? Any local or enterprise projects? Any ideas to launch your personal brand?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1h5shzy', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'HarnessingThePower', 'discussion_type': None, 'num_comments': 37, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5shzy/2025_data_engineering_top_skills_that_you_will/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5shzy/2025_data_engineering_top_skills_that_you_will/', 'subreddit_subscribers': 233985, 'created_utc': 1733246082.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.155+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': "I dont mean to come off as super workaholic or something but is it okay or normal to have days where you don't do anything at all? I am a fairly new DE assigned to a project. There are days where I just wait on the next thing that we need to do, and theres just nothing to do other than upskilling and reading. Is this normal? I still get paid tho so I dont really mind. Thanks in advance!", 'author_fullname': 't2_sr6i193sd', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is it okay to have days where you dont have anything to do?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5e7p9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 80, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 80, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733196792.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I dont mean to come off as super workaholic or something but is it okay or normal to have days where you don&amp;#39;t do anything at all? I am a fairly new DE assigned to a project. There are days where I just wait on the next thing that we need to do, and theres just nothing to do other than upskilling and reading. Is this normal? I still get paid tho so I dont really mind. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1h5e7p9', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Spirited-Ad-9162', 'discussion_type': None, 'num_comments': 33, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5e7p9/is_it_okay_to_have_days_where_you_dont_have/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5e7p9/is_it_okay_to_have_days_where_you_dont_have/', 'subreddit_subscribers': 233985, 'created_utc': 1733196792.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.160+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': "I won a DV lottery (will be a green card holder in 2025) and I'm working as a data engineer in Ukraine. I already started to apply to DE positions in US, but man, what the hell? I applied for like 200 positions already and didn't even get an initial call from a recruiter. I have 4 years of working experience, 2 of them is full time data engineer positions. Is the job market really dead in the US?", 'author_fullname': 't2_t3x9gkla', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "What's happening with DE job market in the US?", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5y3qc', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.79, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 43, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 43, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733259798.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I won a DV lottery (will be a green card holder in 2025) and I&amp;#39;m working as a data engineer in Ukraine. I already started to apply to DE positions in US, but man, what the hell? I applied for like 200 positions already and didn&amp;#39;t even get an initial call from a recruiter. I have 4 years of working experience, 2 of them is full time data engineer positions. Is the job market really dead in the US?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1h5y3qc', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'AsleepLeather5589', 'discussion_type': None, 'num_comments': 39, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5y3qc/whats_happening_with_de_job_market_in_the_us/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5y3qc/whats_happening_with_de_job_market_in_the_us/', 'subreddit_subscribers': 233985, 'created_utc': 1733259798.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.164+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': "Is there an interactive website where you can learn data engineering skills that offers a live experience in a controlled environment? Like maybe learning APIs, Etl, azure  synapse, snowflake etc from a virtual box where you don't have to pay the costs yourself?\n\nThere are plenty of websites that offer this service for other CS skills like tryhackme.com or the hackbox for cyber security, datacamp for data science etc", 'author_fullname': 't2_5ewx5w1k', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is there a website like tryhackme.com for data engineering?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5ju1i', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 36, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 36, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733218936.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there an interactive website where you can learn data engineering skills that offers a live experience in a controlled environment? Like maybe learning APIs, Etl, azure  synapse, snowflake etc from a virtual box where you don&amp;#39;t have to pay the costs yourself?&lt;/p&gt;\n\n&lt;p&gt;There are plenty of websites that offer this service for other CS skills like tryhackme.com or the hackbox for cyber security, datacamp for data science etc&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1h5ju1i', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'OddFirefighter3', 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5ju1i/is_there_a_website_like_tryhackmecom_for_data/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5ju1i/is_there_a_website_like_tryhackmecom_for_data/', 'subreddit_subscribers': 233985, 'created_utc': 1733218936.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.177+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': 'I have a huge dataset of \\~3.5 million JSON files stored on an S3 bucket.  The goal is to do some text analysis, token counts, plot histograms, etc.  \nProblem is the size of the dataset.  It\'s about 87GB:\n\n\\`aws s3 ls s3://my\\_s3\\_bucket/my\\_bucket\\_prefix/ --recursive --human-readable --summarize | grep "Total Size"\\`\n\n&gt;Total Size: 87.2 GiB\n\nIt\'s obviously inefficient to have to re-download all 3.5 million files each time we want to perform some analysis on it.  So the goal is to download all of them *once* and serialize to a data format (I\'m thinking to a \\`.parquet\\` file using gzip or snappy compression).\n\nOnce I\'ve loaded all the json files, I\'ll join them into a Pandas df, and then (crucially, imo) will need to save as parquet somewhere, mainly avoid re-pulling from s3.\n\nProblem is it\'s taking *hours* to pull all these files from S3 in Sagemaker and eventually the Sagemaker notebook just crashes.  So I\'m asking for recommendations on:  \n1.  How to speed up this data fetching and saving to parquet.\n\n2.  If I have any blind-spots that I\'m missing egregiously that I haven\'t considered but should be considering to achieve this.\n\nSince this is an I/O bound task, my plan is to fetch the files in parallel using \\`concurrent.futures.ThreadPoolExecutor\\` to speed up the fetching process.\n\nI\'m currently using a \\`ml.r6i.2xlarge\\` Sagemaker instance, which has 8 vCPUs.  But I plan to run this on a \\`ml.c7i.12xlarge\\` instance with 48 vCPUs.  I expect that should speed up the fetching process by setting the \\`max\\_workers\\` argument to the 48 vCPUs.\n\nOnce I have saved the data to parquet, I plan to use Spark or Dask or Polars to do the analysis if Pandas isn\'t able to handle the large data size.\n\nAppreciate the help and advice.  Thank you.', 'author_fullname': 't2_jsp79xw', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'most efficient way to pull 3.5 million json files from AWS bucket and serialize to parquet file', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5prn8', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 35, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 35, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733239119.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a huge dataset of ~3.5 million JSON files stored on an S3 bucket.  The goal is to do some text analysis, token counts, plot histograms, etc.&lt;br/&gt;\nProblem is the size of the dataset.  It&amp;#39;s about 87GB:&lt;/p&gt;\n\n&lt;p&gt;`aws s3 ls s3://my_s3_bucket/my_bucket_prefix/ --recursive --human-readable --summarize | grep &amp;quot;Total Size&amp;quot;`&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Total Size: 87.2 GiB&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It&amp;#39;s obviously inefficient to have to re-download all 3.5 million files each time we want to perform some analysis on it.  So the goal is to download all of them &lt;em&gt;once&lt;/em&gt; and serialize to a data format (I&amp;#39;m thinking to a `.parquet` file using gzip or snappy compression).&lt;/p&gt;\n\n&lt;p&gt;Once I&amp;#39;ve loaded all the json files, I&amp;#39;ll join them into a Pandas df, and then (crucially, imo) will need to save as parquet somewhere, mainly avoid re-pulling from s3.&lt;/p&gt;\n\n&lt;p&gt;Problem is it&amp;#39;s taking &lt;em&gt;hours&lt;/em&gt; to pull all these files from S3 in Sagemaker and eventually the Sagemaker notebook just crashes.  So I&amp;#39;m asking for recommendations on:&lt;br/&gt;\n1.  How to speed up this data fetching and saving to parquet.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt; If I have any blind-spots that I&amp;#39;m missing egregiously that I haven&amp;#39;t considered but should be considering to achieve this.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Since this is an I/O bound task, my plan is to fetch the files in parallel using `concurrent.futures.ThreadPoolExecutor` to speed up the fetching process.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently using a `ml.r6i.2xlarge` Sagemaker instance, which has 8 vCPUs.  But I plan to run this on a `ml.c7i.12xlarge` instance with 48 vCPUs.  I expect that should speed up the fetching process by setting the `max_workers` argument to the 48 vCPUs.&lt;/p&gt;\n\n&lt;p&gt;Once I have saved the data to parquet, I plan to use Spark or Dask or Polars to do the analysis if Pandas isn&amp;#39;t able to handle the large data size.&lt;/p&gt;\n\n&lt;p&gt;Appreciate the help and advice.  Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1h5prn8', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'johnonymousdenim', 'discussion_type': None, 'num_comments': 45, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5prn8/most_efficient_way_to_pull_35_million_json_files/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5prn8/most_efficient_way_to_pull_35_million_json_files/', 'subreddit_subscribers': 233985, 'created_utc': 1733239119.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.187+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': 'Hey, Any useful tips or experiences you would like to share?', 'author_fullname': 't2_7mnlik68', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Azure Data Engineering - sharing tips', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5nr7b', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 12, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 12, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733233640.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, Any useful tips or experiences you would like to share?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1h5nr7b', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Astherol', 'discussion_type': None, 'num_comments': 19, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5nr7b/azure_data_engineering_sharing_tips/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5nr7b/azure_data_engineering_sharing_tips/', 'subreddit_subscribers': 233985, 'created_utc': 1733233640.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.191+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': 'As subject says, do we have something like leetcode for data engineering? ', 'author_fullname': 't2_o4ceozj5', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Leetcode similar for data engineering', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5pafm', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.85, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733237925.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As subject says, do we have something like leetcode for data engineering? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1h5pafm', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'ExplorerDNA', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5pafm/leetcode_similar_for_data_engineering/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5pafm/leetcode_similar_for_data_engineering/', 'subreddit_subscribers': 233985, 'created_utc': 1733237925.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.194+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': 'Hey folks!\n\nI\'m a self taught DE of 5+ years of exp, and I\'ve reached a point where I find myself limited by the usual "serverless" technologies that I often use for work. I\'d like to expand my horizons and be more flexible in the options I know, to deploy stuff.\n\nCurrently I\'m following [this workshop on ECS](https://catalog.us-east-1.prod.workshops.aws/workshops/8c9036a7-7564-434c-b558-3588754e21f5/en-US) but it\'s mostly "click here, click there, done" type of workshop. DevOps is not my forte, so I\'d like some more explanatory content, that makes me understand what I\'m doing.\n\nAs in the title, do you have specific recommendations for a nice workshop / video playlist / course, that can make me at least nail the foundations of this stuff? \n\nI\'m not aiming to build the new Discord, but at least being able to reliably self-host &lt;insert generic DE tool&gt; in prod, that would be a good start.\n\nCheers!', 'author_fullname': 't2_zwbba', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'A good DevOps/CloudOps course/workshop for busy DEs?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5mrbq', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.85, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733230532.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a self taught DE of 5+ years of exp, and I&amp;#39;ve reached a point where I find myself limited by the usual &amp;quot;serverless&amp;quot; technologies that I often use for work. I&amp;#39;d like to expand my horizons and be more flexible in the options I know, to deploy stuff.&lt;/p&gt;\n\n&lt;p&gt;Currently I&amp;#39;m following &lt;a href="https://catalog.us-east-1.prod.workshops.aws/workshops/8c9036a7-7564-434c-b558-3588754e21f5/en-US"&gt;this workshop on ECS&lt;/a&gt; but it&amp;#39;s mostly &amp;quot;click here, click there, done&amp;quot; type of workshop. DevOps is not my forte, so I&amp;#39;d like some more explanatory content, that makes me understand what I&amp;#39;m doing.&lt;/p&gt;\n\n&lt;p&gt;As in the title, do you have specific recommendations for a nice workshop / video playlist / course, that can make me at least nail the foundations of this stuff? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not aiming to build the new Discord, but at least being able to reliably self-host &amp;lt;insert generic DE tool&amp;gt; in prod, that would be a good start.&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/e8MnKXZtccsr9rqL7zJ9vUnf_FlkXO1M5TgFIm9al4Y.jpg?auto=webp&amp;s=87d1b06df8802b6651c59aa211b34c37fff91a1e', 'width': 501, 'height': 379}, 'resolutions': [{'url': 'https://external-preview.redd.it/e8MnKXZtccsr9rqL7zJ9vUnf_FlkXO1M5TgFIm9al4Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ef7491d01b02b2b76976ce5b724b4aa262c07c1', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/e8MnKXZtccsr9rqL7zJ9vUnf_FlkXO1M5TgFIm9al4Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=35f403c29e2dd7c8b20d2b59a73e1fa5ebd00a91', 'width': 216, 'height': 163}, {'url': 'https://external-preview.redd.it/e8MnKXZtccsr9rqL7zJ9vUnf_FlkXO1M5TgFIm9al4Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=11a89f7118d7a4b31e18c3128ec521af5c88e55d', 'width': 320, 'height': 242}], 'variants': {}, 'id': 'Jn_FpOwniNcMUwAFG-m6L60qh414uM3EQPQL3zg9yV4'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1h5mrbq', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'wtfzambo', 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5mrbq/a_good_devopscloudops_courseworkshop_for_busy_des/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5mrbq/a_good_devopscloudops_courseworkshop_for_busy_des/', 'subreddit_subscribers': 233985, 'created_utc': 1733230532.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.197+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': 'New to data engineeering,\n\nI work as an analytics engineer and mostly work with powerbi to create etl pipelines and create reports. However, the system has been becoming cumbersome with too much transformations. Takes forever to load. We do not have a database and google sheets is our data source. Hence, its fairly easy to connect powerbi to these google sheets, add transformation steps and finally general reports.\n\nI was thinking of implementing azure database system for us to store data and also transform (in azure data factory). Based on my research, It appears you can create data pipeline in azure data factory and connect google sheets and azure sql database. \n\nIf I understand this well, will get azure data factory, azure sql database and azure sql server be enough to create this comprehensive system? What am I missing? we have 100,000 rows every year and have about 3-4years worth data (I believe this is very less). What do you think the cost would be?', 'author_fullname': 't2_j65o4wb4a', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Setting up Azure Database with ETL for a small healthcare company', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5v6j7', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733252710.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New to data engineeering,&lt;/p&gt;\n\n&lt;p&gt;I work as an analytics engineer and mostly work with powerbi to create etl pipelines and create reports. However, the system has been becoming cumbersome with too much transformations. Takes forever to load. We do not have a database and google sheets is our data source. Hence, its fairly easy to connect powerbi to these google sheets, add transformation steps and finally general reports.&lt;/p&gt;\n\n&lt;p&gt;I was thinking of implementing azure database system for us to store data and also transform (in azure data factory). Based on my research, It appears you can create data pipeline in azure data factory and connect google sheets and azure sql database. &lt;/p&gt;\n\n&lt;p&gt;If I understand this well, will get azure data factory, azure sql database and azure sql server be enough to create this comprehensive system? What am I missing? we have 100,000 rows every year and have about 3-4years worth data (I believe this is very less). What do you think the cost would be?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1h5v6j7', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Sufficient_Bug_2716', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5v6j7/setting_up_azure_database_with_etl_for_a_small/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5v6j7/setting_up_azure_database_with_etl_for_a_small/', 'subreddit_subscribers': 233985, 'created_utc': 1733252710.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.205+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': 'For example, market data needs to be real-time which is why companies like Coinbase, Kalshi offer some sort of streaming API over web socket. \n\nWhat are other data feeds that should be real-time? ', 'author_fullname': 't2_12q8c5qc0a', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What datafeeds actually need to be real-time? ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5uldl', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.82, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733251262.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For example, market data needs to be real-time which is why companies like Coinbase, Kalshi offer some sort of streaming API over web socket. &lt;/p&gt;\n\n&lt;p&gt;What are other data feeds that should be real-time? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1h5uldl', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'SorbetIndependent898', 'discussion_type': None, 'num_comments': 16, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5uldl/what_datafeeds_actually_need_to_be_realtime/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5uldl/what_datafeeds_actually_need_to_be_realtime/', 'subreddit_subscribers': 233985, 'created_utc': 1733251262.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.209+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': 'I have only done ETL batch processing in my life. Say I have a DB2 table that I would like to stream into Snowflake, how do I set everything up? Is Snowpipe a feature that requires additional cost and subscription? ', 'author_fullname': 't2_cjk2xje6', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Question about streaming processing', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5q5he', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733240098.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have only done ETL batch processing in my life. Say I have a DB2 table that I would like to stream into Snowflake, how do I set everything up? Is Snowpipe a feature that requires additional cost and subscription? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1h5q5he', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'highlifeed', 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5q5he/question_about_streaming_processing/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5q5he/question_about_streaming_processing/', 'subreddit_subscribers': 233985, 'created_utc': 1733240098.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.214+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': '# [How to create a scalable fraud detection steaming data pipeline](https://medium.com/@rasvihostings/fraud-detection-data-pipeline-etl-on-gcp-2b15b8f3d65b)\n\nWhen we think about large volume streaming data pipeline three things come to our mind\n\n* **Scalability and resilience**\n* **Cost**\n* **Infrastructure maintenance**\n\nI designed a solution which can scale easily, use much as possible GCP managed services and finally reducing the cloud cost 😉\n\n[https://medium.com/@rasvihostings/fraud-detection-data-pipeline-etl-on-gcp-2b15b8f3d65b](https://medium.com/@rasvihostings/fraud-detection-data-pipeline-etl-on-gcp-2b15b8f3d65b)', 'author_fullname': 't2_13kaqr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Fraud detection data pipeline (ETL) on GCP', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5hb0n', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.83, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733207676.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;&lt;a href="https://medium.com/@rasvihostings/fraud-detection-data-pipeline-etl-on-gcp-2b15b8f3d65b"&gt;How to create a scalable fraud detection steaming data pipeline&lt;/a&gt;&lt;/h1&gt;\n\n&lt;p&gt;When we think about large volume streaming data pipeline three things come to our mind&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Scalability and resilience&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Infrastructure maintenance&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I designed a solution which can scale easily, use much as possible GCP managed services and finally reducing the cloud cost 😉&lt;/p&gt;\n\n&lt;p&gt;&lt;a href="https://medium.com/@rasvihostings/fraud-detection-data-pipeline-etl-on-gcp-2b15b8f3d65b"&gt;https://medium.com/@rasvihostings/fraud-detection-data-pipeline-etl-on-gcp-2b15b8f3d65b&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/stIWSTsqOydGfP0ifGjFxHbfNCL3n4BXMlthJzjUz9M.jpg?auto=webp&amp;s=c6c0971ed306449e9502ac132fc29fb675190703', 'width': 1200, 'height': 900}, 'resolutions': [{'url': 'https://external-preview.redd.it/stIWSTsqOydGfP0ifGjFxHbfNCL3n4BXMlthJzjUz9M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bff794460731c96b74a7ab77e089a6069a05df9e', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/stIWSTsqOydGfP0ifGjFxHbfNCL3n4BXMlthJzjUz9M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5fbf26d91737d2c002f41075191ccda296acc1d', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/stIWSTsqOydGfP0ifGjFxHbfNCL3n4BXMlthJzjUz9M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=93faf8ccd6d69ab2826b4eb04295da02cebb08e2', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/stIWSTsqOydGfP0ifGjFxHbfNCL3n4BXMlthJzjUz9M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=90faea0886498c2484806db5dfcd2b98b55fa289', 'width': 640, 'height': 480}, {'url': 'https://external-preview.redd.it/stIWSTsqOydGfP0ifGjFxHbfNCL3n4BXMlthJzjUz9M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=002decec2fe126e44ddac7e7129f9c238658c3ac', 'width': 960, 'height': 720}, {'url': 'https://external-preview.redd.it/stIWSTsqOydGfP0ifGjFxHbfNCL3n4BXMlthJzjUz9M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=798651131869f04d0472a68edb50d229873c56d2', 'width': 1080, 'height': 810}], 'variants': {}, 'id': 'LS0AEPw4oxOvs7ol8ctVSI72ZaOwTa15yRjj-dpy9ao'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1h5hb0n', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'rasvi786', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5hb0n/fraud_detection_data_pipeline_etl_on_gcp/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5hb0n/fraud_detection_data_pipeline_etl_on_gcp/', 'subreddit_subscribers': 233985, 'created_utc': 1733207676.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.221+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': 'Hey! We have an internal web app with a sqlalchemy, alembic, and fastapi setup. I am trying to work on a way to do local testing of the changes to the database layer. Changes such as new tables, new columns, changing constraints, etc...\n\nI would like a workflow where I can test those changes on a database that is the same schema as production. Then I can verify how the data looks coming through the API before committing the changes to the database. Currently, we have 3 different servers with dev, uat, prod in the cloud.\n\nMy current idea to implement a solution is to create a docker-compose file that starts up a local database, leverage pd\\_dump to dump the schema of the prod database and then apply that to the local database. Then the developer can run the new incremental changes on the local database.\n\n  \nWhat are some strategies that you are currently using to solve this problem?', 'author_fullname': 't2_1n3qfa0v', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Internal Web App Databases - How to do local testing', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5ews9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733199046.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! We have an internal web app with a sqlalchemy, alembic, and fastapi setup. I am trying to work on a way to do local testing of the changes to the database layer. Changes such as new tables, new columns, changing constraints, etc...&lt;/p&gt;\n\n&lt;p&gt;I would like a workflow where I can test those changes on a database that is the same schema as production. Then I can verify how the data looks coming through the API before committing the changes to the database. Currently, we have 3 different servers with dev, uat, prod in the cloud.&lt;/p&gt;\n\n&lt;p&gt;My current idea to implement a solution is to create a docker-compose file that starts up a local database, leverage pd_dump to dump the schema of the prod database and then apply that to the local database. Then the developer can run the new incremental changes on the local database.&lt;/p&gt;\n\n&lt;p&gt;What are some strategies that you are currently using to solve this problem?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1h5ews9', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Culpgrant21', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5ews9/internal_web_app_databases_how_to_do_local_testing/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5ews9/internal_web_app_databases_how_to_do_local_testing/', 'subreddit_subscribers': 233985, 'created_utc': 1733199046.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.222+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': 'Hi.\n\nAnyone know of tool options that can replicate a SQL Server on-prem table to Snowflake without having to open/expose SQL Server ports through firewall?  We have Fivetran, but our current plan does not allow this without opening the port in our firewall (would need to upgrade to Enterprise).\n\nWe can unload the data to csv and copy into Snowflake via S3 but this table contains 90+ million rows and may needed daily.\n\nThanks', 'author_fullname': 't2_cymxj8xi', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Tool options which will replicate an on-prem sql server table to Snowflake?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h63ftr', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733273570.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi.&lt;/p&gt;\n\n&lt;p&gt;Anyone know of tool options that can replicate a SQL Server on-prem table to Snowflake without having to open/expose SQL Server ports through firewall?  We have Fivetran, but our current plan does not allow this without opening the port in our firewall (would need to upgrade to Enterprise).&lt;/p&gt;\n\n&lt;p&gt;We can unload the data to csv and copy into Snowflake via S3 but this table contains 90+ million rows and may needed daily.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1h63ftr', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'GreyHairedDWGuy', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h63ftr/tool_options_which_will_replicate_an_onprem_sql/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h63ftr/tool_options_which_will_replicate_an_onprem_sql/', 'subreddit_subscribers': 233985, 'created_utc': 1733273570.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.224+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': "I feel like I'm missing something fundamental here. Has anyone got a CI/CD pipeline set up to deploy Flink jobs written in SQL?  How are you doing it?\n\n  \nThe best I could find was to use the SQL gateway and I'm send queries to it.\n\n  \nMy infra is flink on kubernetes reading from debezium data on kafka, transforming and then sinking to S3", 'author_fullname': 't2_nkc7p', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Flink SQL deployments', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h631ln', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733272462.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like I&amp;#39;m missing something fundamental here. Has anyone got a CI/CD pipeline set up to deploy Flink jobs written in SQL?  How are you doing it?&lt;/p&gt;\n\n&lt;p&gt;The best I could find was to use the SQL gateway and I&amp;#39;m send queries to it.&lt;/p&gt;\n\n&lt;p&gt;My infra is flink on kubernetes reading from debezium data on kafka, transforming and then sinking to S3&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1h631ln', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'sugendran', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h631ln/flink_sql_deployments/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h631ln/flink_sql_deployments/', 'subreddit_subscribers': 233985, 'created_utc': 1733272462.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.225+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': 'How would you recommend to implement row and  column level lineage in a medallion architecture? \n\nWe are using MS Fabric. We have over 60k tables in parquet format within bronze lakehouse. They will then have to be deduplicated and consolidated into cca 10k delta format tables in silver lakehouse. Lastly, we will have cca 500 tables in a warehouse artifact for the golden layer. \n\nWe are using notebooks for bronze and silver layer and then using stored procedures for the golden layer.\n\nThe largest tables have more than 1 billion rows. We thought of creating guid columns, but I assume this would take up a lot of space and would also be computationally expensive. \n\nFor auditing purposes and better data observability we have to implement a data trail, but are not sure on the best way to do so. Any recommendations are more than welcome.', 'author_fullname': 't2_5anfwb0g', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Row and column level data lineage in medallion architecture', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5s3uu', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733245109.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How would you recommend to implement row and  column level lineage in a medallion architecture? &lt;/p&gt;\n\n&lt;p&gt;We are using MS Fabric. We have over 60k tables in parquet format within bronze lakehouse. They will then have to be deduplicated and consolidated into cca 10k delta format tables in silver lakehouse. Lastly, we will have cca 500 tables in a warehouse artifact for the golden layer. &lt;/p&gt;\n\n&lt;p&gt;We are using notebooks for bronze and silver layer and then using stored procedures for the golden layer.&lt;/p&gt;\n\n&lt;p&gt;The largest tables have more than 1 billion rows. We thought of creating guid columns, but I assume this would take up a lot of space and would also be computationally expensive. &lt;/p&gt;\n\n&lt;p&gt;For auditing purposes and better data observability we have to implement a data trail, but are not sure on the best way to do so. Any recommendations are more than welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1h5s3uu', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'merrpip77', 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5s3uu/row_and_column_level_data_lineage_in_medallion/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5s3uu/row_and_column_level_data_lineage_in_medallion/', 'subreddit_subscribers': 233985, 'created_utc': 1733245109.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.227+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': "Hello everyone! I'm curious if any of you are currently exploring\xa0**vector search**\xa0techniques specifically for\xa0**analytical data**. With the recent advancements in platforms like\xa0**Snowflake**\xa0and\xa0**Amazon Redshift**, which are integrating vector search capabilities, I'm interested in hearing your thoughts and experiences.\n\n* Have you implemented vector search in your analytical projects?\n* What challenges have you encountered during implementation?\n* Which vector databases or tools are you utilising (e.g., FAISS, Pinecone, Qdrant)?\n* How do you think vector search will enhance your analytical capabilities moving forward?", 'author_fullname': 't2_kh8g3uy0', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Exploring Vector Search in Analytical Data: Insights and Experiences?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5q3tg', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733239980.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I&amp;#39;m curious if any of you are currently exploring\xa0&lt;strong&gt;vector search&lt;/strong&gt;\xa0techniques specifically for\xa0&lt;strong&gt;analytical data&lt;/strong&gt;. With the recent advancements in platforms like\xa0&lt;strong&gt;Snowflake&lt;/strong&gt;\xa0and\xa0&lt;strong&gt;Amazon Redshift&lt;/strong&gt;, which are integrating vector search capabilities, I&amp;#39;m interested in hearing your thoughts and experiences.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Have you implemented vector search in your analytical projects?&lt;/li&gt;\n&lt;li&gt;What challenges have you encountered during implementation?&lt;/li&gt;\n&lt;li&gt;Which vector databases or tools are you utilising (e.g., FAISS, Pinecone, Qdrant)?&lt;/li&gt;\n&lt;li&gt;How do you think vector search will enhance your analytical capabilities moving forward?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1h5q3tg', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Technical-Pack-5613', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5q3tg/exploring_vector_search_in_analytical_data/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5q3tg/exploring_vector_search_in_analytical_data/', 'subreddit_subscribers': 233985, 'created_utc': 1733239980.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.229+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': '', 'author_fullname': 't2_8weml473', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Databricks Lakehouse vs Traditional Data Warehouse for Multi-Region Gaming Analytics – What’s the best approach?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 72, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5lyro', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.72, 'author_flair_background_color': None, 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/xWbDV_yNrZbfS-IfyZO8gXfaErMmUv-3Efsz581lBsg.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1733227750.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/uklhc38xjm4e1.png', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/uklhc38xjm4e1.png?auto=webp&amp;s=bd5604a57327b4102369ef094697b6d129b6d851', 'width': 1328, 'height': 687}, 'resolutions': [{'url': 'https://preview.redd.it/uklhc38xjm4e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ecf511071dc9d216ebc4da08b751577b63c26d49', 'width': 108, 'height': 55}, {'url': 'https://preview.redd.it/uklhc38xjm4e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed7813c67c3914920cadcfb7c9ee690cf8495d1f', 'width': 216, 'height': 111}, {'url': 'https://preview.redd.it/uklhc38xjm4e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e52db93a8bd21a8cbb1ce4ef18d42cdf80e07723', 'width': 320, 'height': 165}, {'url': 'https://preview.redd.it/uklhc38xjm4e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac961bf6da929823d17617b23aa84e3b9908fdcd', 'width': 640, 'height': 331}, {'url': 'https://preview.redd.it/uklhc38xjm4e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6c9815a797a757c623e40618f605f07737f7e3f7', 'width': 960, 'height': 496}, {'url': 'https://preview.redd.it/uklhc38xjm4e1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=89e655726fd3904e63ad121840fa3426d02fcaaa', 'width': 1080, 'height': 558}], 'variants': {}, 'id': 'FOuAwOaqb-yHTuwPBHFkmj9nHTJgfV6Qvj4dxXWWTmQ'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1h5lyro', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Secret_Walk6385', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5lyro/databricks_lakehouse_vs_traditional_data/', 'stickied': False, 'url': 'https://i.redd.it/uklhc38xjm4e1.png', 'subreddit_subscribers': 233985, 'created_utc': 1733227750.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.237+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': "We have a need to enable a data upload and then quickly present that data in dashboard. The data sets aren't so large as to cause a challenge. But what have been your experiences with using AWS Lambda for this? What other tools are needed to complete this task? We run data into Redshift.", 'author_fullname': 't2_h64zf4jj', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'AWS Lambda Thoughts', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1h64wpz', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'author_cakeday': True, 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733277797.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have a need to enable a data upload and then quickly present that data in dashboard. The data sets aren&amp;#39;t so large as to cause a challenge. But what have been your experiences with using AWS Lambda for this? What other tools are needed to complete this task? We run data into Redshift.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1h64wpz', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Series_G', 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h64wpz/aws_lambda_thoughts/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h64wpz/aws_lambda_thoughts/', 'subreddit_subscribers': 233985, 'created_utc': 1733277797.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.239+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': 'I have a requirement In which I have to find the difference between 2 CSV files and show them in a separate CSV file In the form of Updates, Deletes, Inserts.\nMy initial approach was to use two different pandas dataframe and join them and find the changes - it should appear as - \ninitialValue -&gt; finalValue ( if any cell is changed )\n\nFinding inserts and deletes is pretty easy, but to find which column was changed for a changed row, I basically need to compare two cells of each data frame.\nIt took me around 10 mins to find and create the output file ( 5K rows and 400 columns )\n\nI tried using Spark dataframe thinking it might help but, since file size was of few Mbs, spark was of no help and almost took similar time for above file.\n\nCan you please share your approach? \nI found these Python Library like - csv-diff but it only gives the row which was updated, not any info of which column of that row was updated.\n\nThanks in advance!!', 'author_fullname': 't2_9074ngm7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Best way to find Difference between 2 CSV files', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5tjf0', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733248666.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a requirement In which I have to find the difference between 2 CSV files and show them in a separate CSV file In the form of Updates, Deletes, Inserts.\nMy initial approach was to use two different pandas dataframe and join them and find the changes - it should appear as - \ninitialValue -&amp;gt; finalValue ( if any cell is changed )&lt;/p&gt;\n\n&lt;p&gt;Finding inserts and deletes is pretty easy, but to find which column was changed for a changed row, I basically need to compare two cells of each data frame.\nIt took me around 10 mins to find and create the output file ( 5K rows and 400 columns )&lt;/p&gt;\n\n&lt;p&gt;I tried using Spark dataframe thinking it might help but, since file size was of few Mbs, spark was of no help and almost took similar time for above file.&lt;/p&gt;\n\n&lt;p&gt;Can you please share your approach? \nI found these Python Library like - csv-diff but it only gives the row which was updated, not any info of which column of that row was updated.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1h5tjf0', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'burningpenofasia', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5tjf0/best_way_to_find_difference_between_2_csv_files/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5tjf0/best_way_to_find_difference_between_2_csv_files/', 'subreddit_subscribers': 233985, 'created_utc': 1733248666.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.246+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': "Hi, I have to create a dim\\_calendar table for data mart with key, date, month, year, and quarter.   \nThere are two ways to do this: do a preload for the next x-years or insert a new date row every day (I only need yesterday’s date).\n\nI understand this dim table won't take up so much space if I load it for 10 or 50 years. The question for me is that someone has to remember/see that there are no more dates in dim\\_calendar and add the next x years.\n\nIf I have a script running, I know I always have the date I need. ", 'author_fullname': 't2_fbonkjye', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Calendar Dimension ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5qd36', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733240644.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I have to create a dim_calendar table for data mart with key, date, month, year, and quarter.&lt;br/&gt;\nThere are two ways to do this: do a preload for the next x-years or insert a new date row every day (I only need yesterday’s date).&lt;/p&gt;\n\n&lt;p&gt;I understand this dim table won&amp;#39;t take up so much space if I load it for 10 or 50 years. The question for me is that someone has to remember/see that there are no more dates in dim_calendar and add the next x years.&lt;/p&gt;\n\n&lt;p&gt;If I have a script running, I know I always have the date I need. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1h5qd36', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Dry-Response-1862', 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5qd36/calendar_dimension/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5qd36/calendar_dimension/', 'subreddit_subscribers': 233985, 'created_utc': 1733240644.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.252+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': 'Hi, I’m planning to write GCP certification in hopes of getting a job. I’m trying other things as well - like projects and so on. But I’m spending a lot of money on GCP. Is it worth it in terms of employment or should I do AWS? \n\nThank you. ', 'author_fullname': 't2_rpwoy3vii', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'AWS or GCP DE certification in India? ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5m1zs', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.57, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733228084.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I’m planning to write GCP certification in hopes of getting a job. I’m trying other things as well - like projects and so on. But I’m spending a lot of money on GCP. Is it worth it in terms of employment or should I do AWS? &lt;/p&gt;\n\n&lt;p&gt;Thank you. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1h5m1zs', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Impossible-Alarm-738', 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5m1zs/aws_or_gcp_de_certification_in_india/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5m1zs/aws_or_gcp_de_certification_in_india/', 'subreddit_subscribers': 233985, 'created_utc': 1733228084.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.256+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': "I am trying to build a streaming pipeline on Spark, which would have complicated sessions, being closed on a specific event.\n\nIt is advised to not use windows in this case and fallback to manual state management in flatMapGroupsWithState, so I would be able to manually remove or set a TTL for the state, when I am going to be sure that I won't need it anymore:\n\n[https://www.databricks.com/blog/2021/10/12/native-support-of-session-window-in-spark-structured-streaming.html](https://www.databricks.com/blog/2021/10/12/native-support-of-session-window-in-spark-structured-streaming.html)\n\n&gt;You may still want to leverage flatMapGroupsWithState when your business use case requires a complicated session window, for example, if the case session should also be closed on a specific type of event regardless of inactivity.\n\nBut there is also an implicit state in form of all values for the group:\n\n[https://github.com/apache/spark/blob/branch-3.5/sql/api/src/main/scala/org/apache/spark/sql/streaming/GroupState.scala#L42](https://github.com/apache/spark/blob/branch-3.5/sql/api/src/main/scala/org/apache/spark/sql/streaming/GroupState.scala#L42)\n\nIs there any way to remove this state as well or will it still build up unboundedly?", 'author_fullname': 't2_1c55xifb63', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Is it possible to manually drop groups when using Spark Structured Streaming?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5l9rj', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733225108.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to build a streaming pipeline on Spark, which would have complicated sessions, being closed on a specific event.&lt;/p&gt;\n\n&lt;p&gt;It is advised to not use windows in this case and fallback to manual state management in flatMapGroupsWithState, so I would be able to manually remove or set a TTL for the state, when I am going to be sure that I won&amp;#39;t need it anymore:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href="https://www.databricks.com/blog/2021/10/12/native-support-of-session-window-in-spark-structured-streaming.html"&gt;https://www.databricks.com/blog/2021/10/12/native-support-of-session-window-in-spark-structured-streaming.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;You may still want to leverage flatMapGroupsWithState when your business use case requires a complicated session window, for example, if the case session should also be closed on a specific type of event regardless of inactivity.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;But there is also an implicit state in form of all values for the group:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href="https://github.com/apache/spark/blob/branch-3.5/sql/api/src/main/scala/org/apache/spark/sql/streaming/GroupState.scala#L42"&gt;https://github.com/apache/spark/blob/branch-3.5/sql/api/src/main/scala/org/apache/spark/sql/streaming/GroupState.scala#L42&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Is there any way to remove this state as well or will it still build up unboundedly?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/-g5IrcaHLLpqTr-IrurcJ_ooJsf6Bt6XSgWXI2XdFpo.jpg?auto=webp&amp;s=7330888060677a3384f9d23126dc04d34ac8232c', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/-g5IrcaHLLpqTr-IrurcJ_ooJsf6Bt6XSgWXI2XdFpo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c416d23088fc0eb8e0b8b40688df3a8d236dbf7', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/-g5IrcaHLLpqTr-IrurcJ_ooJsf6Bt6XSgWXI2XdFpo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d3c12a0635445bf039dfb07b205058be42972048', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/-g5IrcaHLLpqTr-IrurcJ_ooJsf6Bt6XSgWXI2XdFpo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f3a95821f21d3b2029eac5ee12c86b13835b8f06', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/-g5IrcaHLLpqTr-IrurcJ_ooJsf6Bt6XSgWXI2XdFpo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0eaa2c11e5101752a13d392931a1369b4a81c25b', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/-g5IrcaHLLpqTr-IrurcJ_ooJsf6Bt6XSgWXI2XdFpo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7d6038c973d8b8a457ea5e791f4a96296dbe3bf9', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/-g5IrcaHLLpqTr-IrurcJ_ooJsf6Bt6XSgWXI2XdFpo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=093d8b3eb7c689256647eb1c53b13d70751e20c3', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'NBFEel2Y96pwwrteq7aPNqIoludK49Y0Q2NbvTiG6qg'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1h5l9rj', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'External-Drag-3815', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5l9rj/is_it_possible_to_manually_drop_groups_when_using/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5l9rj/is_it_possible_to_manually_drop_groups_when_using/', 'subreddit_subscribers': 233985, 'created_utc': 1733225108.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.258+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': 'Hey, a startup gonna give data solutions as a vendor so which you recommend cloud or physical servers \n\n** Note that we are the early beginning**\n\nThe metric iam focusing on :  \n\n- cost  \n- maintenance \n- electricity risk\n-  scalability based on number of customers \n\n\nIf a physical server which on you recommend ', 'author_fullname': 't2_107n0zwsoc', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Cloud vs physical servers for data solution services?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5tizo', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.57, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733248640.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, a startup gonna give data solutions as a vendor so which you recommend cloud or physical servers &lt;/p&gt;\n\n&lt;p&gt;** Note that we are the early beginning**&lt;/p&gt;\n\n&lt;p&gt;The metric iam focusing on :  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;cost&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;maintenance &lt;/li&gt;\n&lt;li&gt;electricity risk&lt;/li&gt;\n&lt;li&gt; scalability based on number of customers &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If a physical server which on you recommend &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1h5tizo', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Murky-Principle6255', 'discussion_type': None, 'num_comments': 11, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5tizo/cloud_vs_physical_servers_for_data_solution/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5tizo/cloud_vs_physical_servers_for_data_solution/', 'subreddit_subscribers': 233985, 'created_utc': 1733248640.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.260+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': '', 'author_fullname': 't2_74rxx', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data science at hackathons supporting integration and open data for digital resilience', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 73, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5rldf', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/NTqqKncQObypR3iKVt69BDzvqXbyO7hln2foN-iLCY4.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1733243807.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'heltweg.org', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.heltweg.org/posts/meta-data-2-oleg-lavrovsky-hacking-integration/', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/mHTGAXVPTLYtd-Tb-jLMJdavglkeD9ASZ0rkMx4FqJ8.jpg?auto=webp&amp;s=8be50b4ef9412e0e203d9a228a0695bf508267a2', 'width': 800, 'height': 420}, 'resolutions': [{'url': 'https://external-preview.redd.it/mHTGAXVPTLYtd-Tb-jLMJdavglkeD9ASZ0rkMx4FqJ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3f7037fe5f786dcc79c4aca407a17824ce217416', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/mHTGAXVPTLYtd-Tb-jLMJdavglkeD9ASZ0rkMx4FqJ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=580045d713f9cac619f0df1d236868f80bf2e0bf', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/mHTGAXVPTLYtd-Tb-jLMJdavglkeD9ASZ0rkMx4FqJ8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=945dc65161e18efd49d2fefbe058bca3caa45650', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/mHTGAXVPTLYtd-Tb-jLMJdavglkeD9ASZ0rkMx4FqJ8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d208ab6a748d8f4a2a5c7bc4b58d29cca7046f6a', 'width': 640, 'height': 336}], 'variants': {}, 'id': '2mn_VXSmESz6wK3umYTDwK4pxMPAbUMoe2MPqAXkSnI'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1h5rldf', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'rhazn', 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5rldf/data_science_at_hackathons_supporting_integration/', 'stickied': False, 'url': 'https://www.heltweg.org/posts/meta-data-2-oleg-lavrovsky-hacking-integration/', 'subreddit_subscribers': 233985, 'created_utc': 1733243807.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.266+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': 'Hello all, I work for a company that has implemented a DBT based solution  solve one of pipeline issues... pretty standard fairs. \n\nOur legacy system involves storing models (sql code) in firestore and calling these through parameterized schedulers and customized self built orchestrators.\n\nNow the powers that be have requested that we keep our DBT solution for orchestration but instead of storing the SQL for the Models and the Seeds on the GIT REPO, they would like to instead store the code and seeds on firestore and retrieve them during the DBT execution. \n\nThere arguments are that it would reduce build costs as the container would only need to be rebuilt when infra changes are made. This also means smaller changes can be made more easily as the whole team can push to production firestore, where as only the lead can push to the production branch on git. \n\nCan anyone see any benefit or upside to this solution because I am really struggling here?', 'author_fullname': 't2_17h92l', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'DBT - Client Request.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5pd9k', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733238095.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, I work for a company that has implemented a DBT based solution  solve one of pipeline issues... pretty standard fairs. &lt;/p&gt;\n\n&lt;p&gt;Our legacy system involves storing models (sql code) in firestore and calling these through parameterized schedulers and customized self built orchestrators.&lt;/p&gt;\n\n&lt;p&gt;Now the powers that be have requested that we keep our DBT solution for orchestration but instead of storing the SQL for the Models and the Seeds on the GIT REPO, they would like to instead store the code and seeds on firestore and retrieve them during the DBT execution. &lt;/p&gt;\n\n&lt;p&gt;There arguments are that it would reduce build costs as the container would only need to be rebuilt when infra changes are made. This also means smaller changes can be made more easily as the whole team can push to production firestore, where as only the lead can push to the production branch on git. &lt;/p&gt;\n\n&lt;p&gt;Can anyone see any benefit or upside to this solution because I am really struggling here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1h5pd9k', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'tcfcfc', 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5pd9k/dbt_client_request/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5pd9k/dbt_client_request/', 'subreddit_subscribers': 233985, 'created_utc': 1733238095.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.275+0000] {logging_mixin.py:188} INFO - {'approved_at_utc': None, 'subreddit': 'dataengineering', 'selftext': "Im new to tech . Pursuing master's in business analytics. With lot of hope . I want to break into data engineering. I want to be as good as someone with 2 years of experience so when Im in job market it's better ( as an international students you need to have some advantage) . I have 1 month winter break coming. Question is where do I start . Tech stack , project ideas . Portfolio project. What should be the road map to be as good as someone with 2 years of experience in data engineering. ", 'author_fullname': 't2_t9fo6l2yz', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Breaking into data engineering ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1h5yacr', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.4, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1733260257.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im new to tech . Pursuing master&amp;#39;s in business analytics. With lot of hope . I want to break into data engineering. I want to be as good as someone with 2 years of experience so when Im in job market it&amp;#39;s better ( as an international students you need to have some advantage) . I have 1 month winter break coming. Question is where do I start . Tech stack , project ideas . Portfolio project. What should be the road map to be as good as someone with 2 years of experience in data engineering. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1h5yacr', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Hungry-Pause6533', 'discussion_type': None, 'num_comments': 11, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1h5yacr/breaking_into_data_engineering/', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1h5yacr/breaking_into_data_engineering/', 'subreddit_subscribers': 233985, 'created_utc': 1733260257.0, 'num_crossposts': 0, 'media': None, 'is_video': False}
[2024-12-04T03:12:14.509+0000] {python.py:201} INFO - Done. Returned value was: /opt/***/data/output/reddit_20241204.csv
[2024-12-04T03:12:14.631+0000] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline, task_id=reddit_extraction, execution_date=20241204T031203, start_date=20241204T031211, end_date=20241204T031214
[2024-12-04T03:12:14.787+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2024-12-04T03:12:14.846+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
